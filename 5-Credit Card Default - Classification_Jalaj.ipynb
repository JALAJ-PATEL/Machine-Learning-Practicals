{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f680a71-17f1-4998-86ae-ecce8b35c480",
   "metadata": {},
   "source": [
    "# Credit Card Defaulter Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40556340-b929-44f1-a0a5-69bf9e442196",
   "metadata": {},
   "source": [
    "## Problem Statement:\n",
    "With the rapid growth of the financial industry, commercial banks face significant threats from credit risks. One of the critical challenges is predicting the likelihood of credit card defaults. This task involves assessing the probability of credit default based on the credit card owner's characteristics and payment history.\n",
    "\n",
    "## Objective:\n",
    "The goal is to develop a machine learning model that can predict the probability of credit default. This model will help banks and financial institutions mitigate risks and make informed lending decisions.\n",
    "\n",
    "## Expected Results:\n",
    "Build a robust solution capable of predicting the probability of credit default, enabling financial institutions to assess risk effectively and make data-driven decisions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ea6d38-652e-44bd-8a41-fd31570743b6",
   "metadata": {},
   "source": [
    "## Step 1: Load and Explore the Dataset\n",
    "\n",
    "In this step, we load the \"UCI_Credit_Card.csv\" dataset using `pandas` and display the first few rows to get an overview of the data structure.\n",
    "We also check for any missing values in the dataset and drop the rows with missing data (this strategy can be modified depending on the dataset).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "696861cf-8c21-447c-a5e4-58809f80a1d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID                            0\n",
      "LIMIT_BAL                     0\n",
      "SEX                           0\n",
      "EDUCATION                     0\n",
      "MARRIAGE                      0\n",
      "AGE                           0\n",
      "PAY_0                         0\n",
      "PAY_2                         0\n",
      "PAY_3                         0\n",
      "PAY_4                         0\n",
      "PAY_5                         0\n",
      "PAY_6                         0\n",
      "BILL_AMT1                     0\n",
      "BILL_AMT2                     0\n",
      "BILL_AMT3                     0\n",
      "BILL_AMT4                     0\n",
      "BILL_AMT5                     0\n",
      "BILL_AMT6                     0\n",
      "PAY_AMT1                      0\n",
      "PAY_AMT2                      0\n",
      "PAY_AMT3                      0\n",
      "PAY_AMT4                      0\n",
      "PAY_AMT5                      0\n",
      "PAY_AMT6                      0\n",
      "default.payment.next.month    0\n",
      "dtype: int64\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 30000 entries, 0 to 29999\n",
      "Data columns (total 25 columns):\n",
      " #   Column                      Non-Null Count  Dtype  \n",
      "---  ------                      --------------  -----  \n",
      " 0   ID                          30000 non-null  int64  \n",
      " 1   LIMIT_BAL                   30000 non-null  float64\n",
      " 2   SEX                         30000 non-null  int64  \n",
      " 3   EDUCATION                   30000 non-null  int64  \n",
      " 4   MARRIAGE                    30000 non-null  int64  \n",
      " 5   AGE                         30000 non-null  int64  \n",
      " 6   PAY_0                       30000 non-null  int64  \n",
      " 7   PAY_2                       30000 non-null  int64  \n",
      " 8   PAY_3                       30000 non-null  int64  \n",
      " 9   PAY_4                       30000 non-null  int64  \n",
      " 10  PAY_5                       30000 non-null  int64  \n",
      " 11  PAY_6                       30000 non-null  int64  \n",
      " 12  BILL_AMT1                   30000 non-null  float64\n",
      " 13  BILL_AMT2                   30000 non-null  float64\n",
      " 14  BILL_AMT3                   30000 non-null  float64\n",
      " 15  BILL_AMT4                   30000 non-null  float64\n",
      " 16  BILL_AMT5                   30000 non-null  float64\n",
      " 17  BILL_AMT6                   30000 non-null  float64\n",
      " 18  PAY_AMT1                    30000 non-null  float64\n",
      " 19  PAY_AMT2                    30000 non-null  float64\n",
      " 20  PAY_AMT3                    30000 non-null  float64\n",
      " 21  PAY_AMT4                    30000 non-null  float64\n",
      " 22  PAY_AMT5                    30000 non-null  float64\n",
      " 23  PAY_AMT6                    30000 non-null  float64\n",
      " 24  default.payment.next.month  30000 non-null  int64  \n",
      "dtypes: float64(13), int64(12)\n",
      "memory usage: 5.7 MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"UCI_Credit_Card.csv\")\n",
    "\n",
    "# Let's display the data\n",
    "df.head()\n",
    "\n",
    "# Check for missing values\n",
    "print(df.isna().sum())\n",
    "\n",
    "# Handle missing values (if any)\n",
    "# For now, let's drop rows with missing values if there are any (can be modified based on strategy)\n",
    "df_cleaned = df.dropna()\n",
    "\n",
    "# Display the cleaned data\n",
    "df_cleaned.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b23f6c4-efa3-4642-b718-d8954e32412a",
   "metadata": {},
   "source": [
    "## Step 2: Feature Selection\n",
    "\n",
    "Here, we separate the target variable `default.payment.next.month` from the other features. Using `SelectKBest` with the ANOVA F-test (`f_classif`), we select the top 8 most important features that are strongly correlated with the target. These selected features will be used for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25c835be-45be-4dc7-805b-4f52658e61ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['LIMIT_BAL', 'PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6',\n",
      "       'PAY_AMT1'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# Separating the target variable (default payment) and features\n",
    "X = df_cleaned.drop(columns=['default.payment.next.month', 'ID'])\n",
    "y = df_cleaned['default.payment.next.month']\n",
    "\n",
    "# Applying SelectKBest to pick the top 8 features\n",
    "selector = SelectKBest(score_func=f_classif, k=8)\n",
    "X_new = selector.fit_transform(X, y)\n",
    "\n",
    "# Display the selected features\n",
    "selected_features = selector.get_support(indices=True)\n",
    "print(X.columns[selected_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2601ce5-03f9-4747-acb6-80b15ac41d0b",
   "metadata": {},
   "source": [
    "## Step 3: Feature Scaling\n",
    "\n",
    "In this step, we standardize the selected features using `StandardScaler` to ensure that all features are on the same scale. This is crucial for machine learning algorithms such as SVM and KNN, which are sensitive to feature scales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c42f21a-5ef6-47d4-a808-56e6e8a07387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.13672015  1.79456386  1.78234817 -0.69666346 -0.66659873 -1.53004603\n",
      "  -1.48604076 -0.34194162]\n",
      " [-0.3659805  -0.87499115  1.78234817  0.1388648   0.18874609  0.23491652\n",
      "   1.99231551 -0.34194162]\n",
      " [-0.59720239  0.01486052  0.1117361   0.1388648   0.18874609  0.23491652\n",
      "   0.25313738 -0.25029158]\n",
      " [-0.90549825  0.01486052  0.1117361   0.1388648   0.18874609  0.23491652\n",
      "   0.25313738 -0.22119058]\n",
      " [-0.90549825 -0.87499115  0.1117361  -0.69666346  0.18874609  0.23491652\n",
      "   0.25313738 -0.22119058]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Scaling the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_new)\n",
    "\n",
    "# Display the scaled data\n",
    "print(X_scaled[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c9009d-49f2-409a-b0d2-584be8b0d9cf",
   "metadata": {},
   "source": [
    "## Step 4: Split the Data for Training and Testing\n",
    "\n",
    "In this step, we split the scaled features and target variables into training and testing datasets. We allocate `70% of the data to training` and `30% to testing`, ensuring that we maintain a `random state of 42` for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68f7351f-1a11-454c-a7ce-90ea666c8f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128c1bf2-2bba-4966-833e-4e8b4abfd304",
   "metadata": {},
   "source": [
    "## Step 5: Perform Model Training with Cross-Validation and Grid Search\n",
    "In this step, we train the models using **Grid Search with 5-fold Cross-Validation** to optimize hyperparameters. Grid Search helps us select the best model by trying different combinations of hyperparameters and choosing the one that yields the best performance. \n",
    "\n",
    "The models evaluated include:\n",
    "\n",
    "1. **Logistic Regression**: We vary the penalty type (L1/L2) and the regularization strength (C).\n",
    "2. **Decision Tree**: We tune the maximum depth and minimum samples required to split a node.\n",
    "3. **K-Nearest Neighbors (KNN)**: We test different numbers of neighbors, weight functions, and distance metrics.\n",
    "4. **Support Vector Machine (SVM)**: We try different regularization strengths (C) and kernel types (linear and RBF).\n",
    "\n",
    "We evaluate each model using precision, recall, accuracy, and F1-Score, and finally, we select the best model based on the highest F1-Score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfe9f5a-fb69-4387-8185-5e869f5be635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing Grid Search for Logistic Regression...\n",
      "Performing Grid Search for Decision Tree...\n",
      "Performing Grid Search for KNN...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jpate\\AppData\\Roaming\\Python\\Python312\\site-packages\\numpy\\ma\\core.py:2846: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing Grid Search for SVM...\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define models and hyperparameter grids for GridSearchCV\n",
    "param_grids = {\n",
    "    \"Logistic Regression\": {\n",
    "        'penalty': ['l1', 'l2'],\n",
    "        'C': [0.01, 0.1, 1, 10]\n",
    "    },\n",
    "    \"Decision Tree\": {\n",
    "        'max_depth': [5, 10, 15],\n",
    "        'min_samples_split': [2, 5, 10]\n",
    "    },\n",
    "    \"KNN\": {\n",
    "        'n_neighbors': [3, 5, 7],\n",
    "        'weights': ['uniform', 'distance'],\n",
    "        'metric': ['euclidean', 'manhattan']\n",
    "    },\n",
    "    \"SVM\": {\n",
    "        'C': [0.1, 1, 10],\n",
    "        'kernel': ['linear', 'rbf']\n",
    "    }\n",
    "}\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(solver='liblinear'),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"KNN\": KNeighborsClassifier(),\n",
    "    \"SVM\": SVC()\n",
    "}\n",
    "\n",
    "# Dictionary to store the results\n",
    "results = {}\n",
    "\n",
    "# Function to perform Grid Search and evaluate the model\n",
    "def evaluate_model_with_grid_search(model, param_grid, X_train, X_test, y_train, y_test):\n",
    "    # Perform Grid Search with 5-fold cross-validation\n",
    "    grid_search = GridSearchCV(model, param_grid, cv=5, scoring='f1', n_jobs=-1)\n",
    "    \n",
    "    # Fit the model\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Get the best model\n",
    "    best_model = grid_search.best_estimator_\n",
    "    \n",
    "    # Make predictions on the test set\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    # Return the best parameters and the metrics\n",
    "    return {\n",
    "        \"Best Params\": grid_search.best_params_,\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"F1-Score\": f1\n",
    "    }\n",
    "\n",
    "# Perform Grid Search for each model and evaluate\n",
    "for model_name, model in models.items():\n",
    "    print(f\"Performing Grid Search for {model_name}...\")\n",
    "    results[model_name] = evaluate_model_with_grid_search(model, param_grids[model_name], X_train, X_test, y_train, y_test)\n",
    "\n",
    "# Display results for all models\n",
    "for model_name, metrics in results.items():\n",
    "    print(f\"\\n{model_name} Results:\")\n",
    "    print(f\"Best Hyperparameters: {metrics['Best Params']}\")\n",
    "    for metric_name, score in metrics.items():\n",
    "        if metric_name != \"Best Params\":\n",
    "            print(f\"{metric_name}: {score:.4f}\")\n",
    "\n",
    "# Find the best model based on F1-Score\n",
    "best_model = max(results, key=lambda x: results[x]['F1-Score'])\n",
    "print(f\"\\nBest model based on F1-Score: {best_model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5479b46-eb40-4e00-804a-fc04719d3a76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
